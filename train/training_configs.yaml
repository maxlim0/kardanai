mac_config:
  name: "mac"
  num_train_epochs: 1
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 2
  eval_strategy: "steps"
  eval_steps: 70
  save_steps: 0 # сохраним один чекпоинт в конце
  learning_rate: 1.0e-5
  weight_decay: 0.01
  fp16: false
  use_cpu: false
  warmup_ratio: 0.1
  save_total_limit: 1
  do_train: true
  do_eval: true
  disable_tqdm: false
  logging_first_step: true
  logging_steps: 1
  overwrite_output_dir: true
  logging_dir: "data/model/logs"
  report_to: ["tensorboard", "neptune"]
  output_dir: "data/model/llama_lora_output"
  load_best_model_at_end: true
  metric_for_best_model: eval_loss
  greater_is_better: false


h100_config:
  name: "h100"
  num_train_epochs: 20 # ~13 эпох в час; 62,5 батча на эпоху; 1250 шагов
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 4
  eval_strategy: "steps"
  eval_steps: 200
  save_steps: 200
  save_total_limit: 3
  learning_rate: 1.0e-5
  weight_decay: 0.01
  fp16: true
  use_cpu: false
  warmup_ratio: 0.1
  do_train: true
  do_eval: true
  disable_tqdm: false
  logging_first_step: true
  logging_steps: 5
  overwrite_output_dir: true
  logging_dir: "data/model/logs"
  report_to: ["tensorboard", "neptune"]
  output_dir: "data/model/llama_lora_output"
  load_best_model_at_end: true,
  metric_for_best_model: eval_loss,
  greater_is_better: false,
